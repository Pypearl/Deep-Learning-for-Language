{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"d6dfb3daad9163bb09d54a6aa23220c86da4f1d64708f67079eb8a8e29fa092c"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction to Natural Language Processing 2 Lab01\n\nIn this lab, we are going to implement a pretty decent machine translation model using the transformer and then compare it with differents decoding functions.","metadata":{"id":"KHnspT3ZSHfC"}},{"cell_type":"markdown","source":"---\n## Installation","metadata":{"id":"hx1Av-RfBlU6"}},{"cell_type":"code","source":"%matplotlib inline","metadata":{"id":"AtYWnxIFo4EI","execution":{"iopub.status.busy":"2022-11-17T22:32:42.573148Z","iopub.execute_input":"2022-11-17T22:32:42.574056Z","iopub.status.idle":"2022-11-17T22:32:42.606123Z","shell.execute_reply.started":"2022-11-17T22:32:42.573940Z","shell.execute_reply":"2022-11-17T22:32:42.605333Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Let's install required packages for the project.","metadata":{"id":"UDQTd1FvcPIL"}},{"cell_type":"code","source":"!pip install -U torchtext==0.12 torchdata==0.3.0\n!pip install -U spacy sacrebleu\n!python -m spacy download en_core_web_sm\n!python -m spacy download de_core_news_sm","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tYHTOZJvo5ec","outputId":"c4b1889e-847f-4407-ad7f-5f4e4a1c8d04","execution":{"iopub.status.busy":"2022-11-17T22:32:42.618069Z","iopub.execute_input":"2022-11-17T22:32:42.618321Z","iopub.status.idle":"2022-11-17T22:34:10.339708Z","shell.execute_reply.started":"2022-11-17T22:32:42.618297Z","shell.execute_reply":"2022-11-17T22:34:10.338534Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchtext==0.12 in /opt/conda/lib/python3.7/site-packages (0.12.0)\nCollecting torchdata==0.3.0\n  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12) (2.28.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12) (1.21.6)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext==0.12) (1.11.0)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (1.26.12)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.12) (4.1.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.12) (3.3)\nInstalling collected packages: torchdata\nSuccessfully installed torchdata-0.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (3.3.1)\nCollecting spacy\n  Downloading spacy-3.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting sacrebleu\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (21.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.4.5)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.6.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (59.8.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.8)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.7)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.3)\nCollecting thinc<8.2.0,>=8.1.0\n  Downloading thinc-8.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (806 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.9)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.3.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.1.2)\nRequirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.4.2)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.8)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.28.1)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.8.2)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.21.6)\nRequirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy) (4.1.1)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.10.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.10)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (4.64.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.6.0)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (4.9.1)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.5)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2021.11.10)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy) (2.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy) (4.13.0)\nInstalling collected packages: sacrebleu, thinc, spacy\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.0.17\n    Uninstalling thinc-8.0.17:\n      Successfully uninstalled thinc-8.0.17\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.3.1\n    Uninstalling spacy-3.3.1:\n      Successfully uninstalled spacy-3.3.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nen-core-web-sm 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 3.4.3 which is incompatible.\nen-core-web-lg 3.3.0 requires spacy<3.4.0,>=3.3.0.dev0, but you have spacy 3.4.3 which is incompatible.\nallennlp 2.10.1 requires spacy<3.4,>=2.1.0, but you have spacy 3.4.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed sacrebleu-2.3.1 spacy-3.4.3 thinc-8.1.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m2022-11-17 22:33:20.465950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:20.466902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:20.467927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:20.468856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:20.469697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:20.470516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nCollecting en-core-web-sm==3.4.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.8.2)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\nRequirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (59.8.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\nRequirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\nRequirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.12)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.13.0)\nInstalling collected packages: en-core-web-sm\n  Attempting uninstall: en-core-web-sm\n    Found existing installation: en-core-web-sm 3.3.0\n    Uninstalling en-core-web-sm-3.3.0:\n      Successfully uninstalled en-core-web-sm-3.3.0\nSuccessfully installed en-core-web-sm-3.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n2022-11-17 22:33:51.419538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:51.420452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:51.421451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:51.422267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:51.423053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:33:51.423824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nCollecting de-core-news-sm==3.4.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from de-core-news-sm==3.4.0) (3.4.3)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.8.2)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.7)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.28.1)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.9)\nRequirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.1.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\nRequirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (59.8.0)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.6.2)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.8)\nRequirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.4.2)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.21.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.1.0)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.9)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.1.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.13.0)\nInstalling collected packages: de-core-news-sm\nSuccessfully installed de-core-news-sm-3.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('de_core_news_sm')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--- \n# Language Translation with nn.Transformer and torchtext\n\nTo start with, we just follow the pyTorch [language translation with nn.Transformer and torchtext tutorial.](https://pytorch.org/tutorials/beginner/translation_transformer.html)\n\nThis tutorial shows:\n- How to train a translation model from scratch using Transformer. \n- Use tochtext library to access  [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1) dataset to train a German to English translation model.\n","metadata":{"id":"IcH0wYwTo4EK"}},{"cell_type":"markdown","source":"## Data Sourcing and Processing\n\n[torchtext library](https://pytorch.org/text/stable/) has utilities for creating datasets that can be easily\niterated through for the purposes of creating a language translation\nmodel. In this example, we show how to use torchtext's inbuilt datasets, \ntokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n[Multi30k dataset from torchtext library](https://pytorch.org/text/stable/datasets.html#multi30k) that yields a pair of source-target raw sentences. ","metadata":{"id":"X9t7Fbomo4EL"}},{"cell_type":"code","source":"from torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.datasets import multi30k, Multi30k\nfrom typing import Iterable, List, Tuple, Callable\n\n# We need to modify the URLs for the dataset since the links to the original dataset are broken\n# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\nmulti30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\nmulti30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n\nSRC_LANGUAGE = 'de'\nTGT_LANGUAGE = 'en'\n\n# Place-holders\ntoken_transform = {}\nvocab_transform = {}\n\n# Create source and target language tokenizer. Make sure to install the dependencies.\ntoken_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\ntoken_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n\ndef yield_tokens(data_iter: List[str], language: str) -> List[str]:\n    \"\"\"\n    Helper function to yield list of tokens.\n\n    Parameters\n    ----------\n    data_iter: List[str]\n        list of tokens to yield\n    language: str\n        destination language\n    \n    Returns\n    -------\n    List[str]\n    the yielded tokens' list\n    \"\"\"\n    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n\n    for data_sample in data_iter:\n        yield token_transform[language](data_sample[language_index[language]])\n\n# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n \nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    # Training data Iterator \n    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    # Create torchtext's Vocab object \n    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n                                                    min_freq=1,\n                                                    specials=special_symbols,\n                                                    special_first=True)\n\n# Set UNK_IDX as the default index. This index is returned when the token is not found. \n# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n  vocab_transform[ln].set_default_index(UNK_IDX)","metadata":{"id":"t6x8zTkRo4EL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"88f8e06b-d657-42b6-d8d8-71f3d4dff752","execution":{"iopub.status.busy":"2022-11-17T22:34:10.342084Z","iopub.execute_input":"2022-11-17T22:34:10.342478Z","iopub.status.idle":"2022-11-17T22:34:28.908746Z","shell.execute_reply.started":"2022-11-17T22:34:10.342426Z","shell.execute_reply":"2022-11-17T22:34:28.907715Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2022-11-17 22:34:13.278739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:34:13.280016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:34:13.281353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:34:13.282401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:34:13.283401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-17 22:34:13.284189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/datapipes/iter/combining.py:181: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Seq2Seq Network using Transformer\n\nTransformer is a Seq2Seq model introduced in [“Attention is all you\nneed”](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)_\npaper for solving machine translation tasks. \nBelow, we will create a Seq2Seq network that uses Transformer. The network\nconsists of three parts. First part is the embedding layer. This layer converts tensor of input indices\ninto corresponding tensor of input embeddings. These embedding are further augmented with positional\nencodings to provide position information of input tokens to the model. The second part is the \nactual [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ model. \nFinally, the output of Transformer model is passed through linear layer\nthat give un-normalized probabilities for each token in the target language. \n\n\n","metadata":{"id":"n0dT8410o4EM"}},{"cell_type":"code","source":"from torch import Tensor\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\nimport math\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Fix random state to get reproductible results\ntorch.random.manual_seed(42)\nimport random\nrandom.seed(42)\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n    \n    Attributes\n    ----------\n    dropout: torch.nn.Module\n        dropout layer\n\n    Methods\n    -------\n    forward(self, token_embedding: Tensor) -> Tensor\n        Neural network module forwarding function\n    \"\"\"\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000) -> None:\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor) -> Tensor:\n        \"\"\"\n        Neural network module forwarding function\n\n        Parameters\n        ----------\n        tokens: Tensor\n            tokens tensor\n\n        Return\n        ------\n        Tensor\n        Tensor of tokens after dropout\n        \"\"\"\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\nclass TokenEmbedding(nn.Module):\n    \"\"\"\n    Helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n    \n    Attributes\n    ----------\n    embedding: torch.nn.Module\n        embedding layer\n\n    Methods\n    -------\n    forward(self, token_embedding: Tensor) -> Tensor\n        Neural network module forwarding function\n    \"\"\"\n    \n    def __init__(self, vocab_size: int, emb_size) -> None:\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor) -> Tensor:\n        \"\"\"\n        Neural network module forwarding function\n\n        Parameters\n        ----------\n        tokens: Tensor\n            tokens tensor\n\n        Return\n        ------\n        Tensor\n        Tensor of tokens embedding after forward\n        \"\"\"\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\nclass Seq2SeqTransformer(nn.Module):\n    \"\"\"\n    Seq2Seq Network \n    \n    Attributes\n    ----------\n    transformer: torch.nn.Module\n        dropout layer\n\n    Methods\n    -------\n    forward(self, token_embedding: Tensor) -> Tensor\n        Neural network module forwarding function\n    \"\"\"\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1) -> None:\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor) -> Tensor:\n        \"\"\"\n        Seq2Seq module forwarding function\n\n        Parameters\n        ----------\n        src: Tensor\n            source tokens tensor\n        trg: Tensor\n            target tokens tensor\n        src_mask: Tensor\n            mask for source tokens\n        tgt_mask: Tensor\n            mask for target tokens\n        src_padding_mask: Tensor\n            padding mask for source tokens\n        tgt_padding_mask: Tensor\n            padding mask for target tokens\n        memory_key_padding_mask: Tensor\n            memory of padding_mask for transformer\n\n        Return\n        ------\n        Tensor\n        Generated outputs after applying transformer on source and target embeddings\n        \"\"\"\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n        \"\"\"\n        Encode source tokens by applying transformer\n\n        Parameters\n        ----------\n        src: Tensor\n            source tokens\n        src_mask: Tensor\n            mask for source tokens\n\n        Return\n        ------\n        Tensor\n        Encoded tensor of source tokens\n        \"\"\"\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor) -> Tensor:\n        \"\"\"\n        Decode target tokens by applying transformer decoder \n\n        Parameters\n        ----------\n        tgt: Tensor\n            target tokens\n        memory: Tensor\n            memory tensor use by decoder\n        tgt_mask: Tensor\n            mask for target tokens\n        \n        Return\n        ------\n        Tensor\n        Decoded tensor of target tokens\n        \"\"\"\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)","metadata":{"id":"JzA6PtyMo4EM","execution":{"iopub.status.busy":"2022-11-17T23:24:42.495671Z","iopub.execute_input":"2022-11-17T23:24:42.496128Z","iopub.status.idle":"2022-11-17T23:24:42.528463Z","shell.execute_reply.started":"2022-11-17T23:24:42.496089Z","shell.execute_reply":"2022-11-17T23:24:42.527282Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"markdown","source":"During training, we need a subsequent word mask that will prevent model to look into\nthe future words when making predictions. We will also need masks to hide\nsource and target padding tokens. Below, let's define a function that will take care of both. \n\n\n","metadata":{"id":"SdgZR7t1o4EN"}},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz: int) -> Tensor:\n    \"\"\"\n    Generate a subsequent mask that will prevent model to look into the future words when making predictions\n\n    Parameters\n    ----------\n    sz: int\n        sequence length\n    \n    Return\n    ------\n    Tensor\n    Subsequent mask\n    \"\"\"\n    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src : Tensor, tgt: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    \"\"\"\n    Create mask and padding mask for source and target tokens\n\n    Parameters\n    ----------\n    src: Tensor\n        source tokens\n    tgt: Tensor\n        target tokens\n\n    Return\n    ------\n    Tuple[Tensor, Tensor, Tensor, Tensor]\n    mask for source tokens, mask for target tokens, padding mask for source tokens and padding mask for target tokens\n    \"\"\"\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask","metadata":{"id":"g__M3Qo4o4EN","execution":{"iopub.status.busy":"2022-11-17T22:34:28.935726Z","iopub.execute_input":"2022-11-17T22:34:28.936103Z","iopub.status.idle":"2022-11-17T22:34:28.946175Z","shell.execute_reply.started":"2022-11-17T22:34:28.936068Z","shell.execute_reply":"2022-11-17T22:34:28.945200Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Let's now define the parameters of our model and instantiate it. Below, we also \ndefine our loss function which is the cross-entropy loss and the optimizer used for training, here Adam.\n\n\n","metadata":{"id":"f9nFe_b-o4EO"}},{"cell_type":"code","source":"torch.manual_seed(0)\n\nSRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\nTGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nBATCH_SIZE = 128\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n\ntransformer = transformer.to(DEVICE)\n\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\noptimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)","metadata":{"id":"q22Hw4Ano4EO","execution":{"iopub.status.busy":"2022-11-17T22:34:28.947442Z","iopub.execute_input":"2022-11-17T22:34:28.948020Z","iopub.status.idle":"2022-11-17T22:34:29.649409Z","shell.execute_reply.started":"2022-11-17T22:34:28.947984Z","shell.execute_reply":"2022-11-17T22:34:29.648296Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Collation\n\nAs seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings. \nWe need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network \ndefined previously. Below we define our collate function that convert batch of raw strings into batch tensors that\ncan be fed directly into our model.   \n\n\n","metadata":{"id":"__b0xyjWo4EO"}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef sequential_transforms(*transforms: Iterable) -> Callable:\n    \"\"\"\n    Helper function to club together sequential operations\n\n    Parameters\n    ----------\n\n    transforms: Iterable\n        callable function iterator\n\n    Return\n    ------\n    Callable\n    Callable function to apply all sequential operations on a text input\n    \"\"\"\n    def func(txt_input):\n        for transform in transforms:\n            txt_input = transform(txt_input)\n        return txt_input\n    return func\n\ndef tensor_transform(token_ids: List[int]) -> Tensor:\n    \"\"\"\n    Function to add BOS/EOS and create tensor for input sequence indices\n\n    Parameters\n    ----------\n    token_ids: List[int]\n        token indices\n    \n    Return\n    ------\n    Tensor\n    Final tensor with BOS/EOS added\n    \"\"\"\n    return torch.cat((torch.tensor([BOS_IDX]), \n                      torch.tensor(token_ids), \n                      torch.tensor([EOS_IDX])))\n\n# src and tgt language text transforms to convert raw strings into tensors indices\ntext_transform = {}\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n                                               vocab_transform[ln], #Numericalization\n                                               tensor_transform) # Add BOS/EOS and create tensor\n\n\ndef collate_fn(batch: List) -> Tuple[Tensor, Tensor]:\n    \"\"\"\n    Function to collate data samples into batch tensors\n\n    Parameters\n    ----------\n    batch: List[Tensor]\n        batch of tensor to collate\n\n    Return\n    ------\n    Tuple[Tensor, Tensor]\n    source batch and target batch that can be fed into our model\n    \"\"\"\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n\n    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n    return src_batch, tgt_batch","metadata":{"id":"ph0qzayeo4EO","execution":{"iopub.status.busy":"2022-11-17T22:34:29.650967Z","iopub.execute_input":"2022-11-17T22:34:29.651369Z","iopub.status.idle":"2022-11-17T22:34:29.661517Z","shell.execute_reply.started":"2022-11-17T22:34:29.651327Z","shell.execute_reply":"2022-11-17T22:34:29.660455Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Let's define training and evaluation loop that will be called for each \nepoch.\n\n\n","metadata":{"id":"AR3JSOTwo4EP"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef train_epoch(model : torch.nn.Module, optimizer: torch.optim) -> float:\n    \"\"\"\n    Train function for a model used for each epoch\n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        model to train\n    optimizer: torch.optim\n        optimizer to use for training\n    \n    Return\n    ------\n    float\n    Training loss\n    \"\"\"\n    model.train()\n    losses = 0\n    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n    \n    for src, tgt in train_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        optimizer.zero_grad()\n\n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        loss.backward()\n\n        optimizer.step()\n        losses += loss.item()\n\n    return losses / len(list(train_dataloader))\n\n\ndef evaluate(model: torch.nn.Module) -> float:\n    \"\"\"\n    Evaluation function for a model used for each epoch\n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        model to evaluate\n\n    Return\n    ------\n    float\n    Validation loss\n    \"\"\"\n    model.eval()\n    losses = 0\n\n    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\n    for src, tgt in val_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n        \n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n\n    return losses / len(list(val_dataloader))","metadata":{"id":"j-8QskQ8o4EP","execution":{"iopub.status.busy":"2022-11-17T22:34:29.663395Z","iopub.execute_input":"2022-11-17T22:34:29.664135Z","iopub.status.idle":"2022-11-17T22:34:29.680436Z","shell.execute_reply.started":"2022-11-17T22:34:29.664080Z","shell.execute_reply":"2022-11-17T22:34:29.679278Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Now we have all the ingredients to train our model. Let's do it !  \nWe choose to train on 20 epochs for our results.\n\n\n","metadata":{"id":"dKi4Z75wo4EP"}},{"cell_type":"code","source":"from timeit import default_timer as timer\nNUM_EPOCHS = 20\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    print((f\"Epoch {epoch:<2}: Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"o8RMv3ZFxuj-","outputId":"8b31dc95-44fe-4cd8-e8a5-2c312b26b950","execution":{"iopub.status.busy":"2022-11-17T22:34:29.681994Z","iopub.execute_input":"2022-11-17T22:34:29.682577Z","iopub.status.idle":"2022-11-17T22:49:37.536210Z","shell.execute_reply.started":"2022-11-17T22:34:29.682540Z","shell.execute_reply":"2022-11-17T22:49:37.535127Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1 : Train loss: 5.344, Val loss: 4.114, Epoch time = 42.887s\nEpoch 2 : Train loss: 3.761, Val loss: 3.320, Epoch time = 43.852s\nEpoch 3 : Train loss: 3.162, Val loss: 2.895, Epoch time = 44.492s\nEpoch 4 : Train loss: 2.768, Val loss: 2.637, Epoch time = 44.752s\nEpoch 5 : Train loss: 2.481, Val loss: 2.442, Epoch time = 44.597s\nEpoch 6 : Train loss: 2.250, Val loss: 2.317, Epoch time = 44.455s\nEpoch 7 : Train loss: 2.060, Val loss: 2.200, Epoch time = 45.140s\nEpoch 8 : Train loss: 1.897, Val loss: 2.112, Epoch time = 44.557s\nEpoch 9 : Train loss: 1.754, Val loss: 2.060, Epoch time = 45.015s\nEpoch 10: Train loss: 1.631, Val loss: 2.000, Epoch time = 45.844s\nEpoch 11: Train loss: 1.524, Val loss: 1.975, Epoch time = 44.672s\nEpoch 12: Train loss: 1.420, Val loss: 1.944, Epoch time = 44.455s\nEpoch 13: Train loss: 1.333, Val loss: 1.966, Epoch time = 45.275s\nEpoch 14: Train loss: 1.252, Val loss: 1.941, Epoch time = 44.875s\nEpoch 15: Train loss: 1.173, Val loss: 1.925, Epoch time = 44.448s\nEpoch 16: Train loss: 1.103, Val loss: 1.914, Epoch time = 45.330s\nEpoch 17: Train loss: 1.038, Val loss: 1.896, Epoch time = 44.602s\nEpoch 18: Train loss: 0.978, Val loss: 1.906, Epoch time = 44.583s\nEpoch 19: Train loss: 0.920, Val loss: 1.900, Epoch time = 43.900s\nEpoch 20: Train loss: 0.866, Val loss: 1.927, Epoch time = 44.243s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--- \n## Decoding functions\n\nTo use our trained model to translate, we need decoding functions.\n\n### Greedy algorithm\nThe tutorial give a greedy approach at decoding. Let's implement it and test our model with it first.","metadata":{"id":"XxLap9aOlvh9"}},{"cell_type":"code","source":"def greedy_decode(model: torch.nn.Module, src: Tensor, src_mask: Tensor, max_len: int, start_symbol: int) -> Tensor:\n    \"\"\"\n    Function to generate output sequence using greedy algorithm \n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        model to decode\n    src: Tensor\n        source tokens\n    src_mask: Tensor\n        mask source tokens\n    max_len: int\n        number of tokens\n    start_symbol: int\n        first symbol of output sequence\n    \n    Return\n    ------\n    Tensor\n    Output sequence\n    \"\"\"\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.item()\n\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys","metadata":{"id":"QXhG3iExo4EP","execution":{"iopub.status.busy":"2022-11-17T22:53:38.547784Z","iopub.execute_input":"2022-11-17T22:53:38.548928Z","iopub.status.idle":"2022-11-17T22:53:38.559291Z","shell.execute_reply.started":"2022-11-17T22:53:38.548851Z","shell.execute_reply":"2022-11-17T22:53:38.558179Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def translate_greedy(model: torch.nn.Module, src_sentence: str) -> str:\n    \"\"\"\n    Function to translate input sentence into target language with greedy algorithm\n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        pre-trained model used for translate\n    src_sentence: str\n        sentence to translate\n    \n    Return\n    ------\n    str\n    Translated sentence in the target language of the transformer (model)\n    \"\"\"\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")","metadata":{"id":"STVSiZsE3Md0","execution":{"iopub.status.busy":"2022-11-17T22:53:39.087898Z","iopub.execute_input":"2022-11-17T22:53:39.088523Z","iopub.status.idle":"2022-11-17T22:53:39.095890Z","shell.execute_reply.started":"2022-11-17T22:53:39.088483Z","shell.execute_reply":"2022-11-17T22:53:39.094896Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"print(translate_greedy(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu.\"))","metadata":{"id":"Dqqy13rBo4EQ","execution":{"iopub.status.busy":"2022-11-17T22:53:39.268637Z","iopub.execute_input":"2022-11-17T22:53:39.269512Z","iopub.status.idle":"2022-11-17T22:53:39.328427Z","shell.execute_reply.started":"2022-11-17T22:53:39.269471Z","shell.execute_reply":"2022-11-17T22:53:39.327429Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":" A group of people standing in front of an igloo \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Top K sampling algorithm\n\nTo compare our result with the greedy algorithm, let's implement a top k sampling algorithm with and without temperature.","metadata":{"id":"CB271z4IONjX"}},{"cell_type":"code","source":"def top_k_sampling(model: torch.nn.Module, src: Tensor, src_mask: Tensor, max_len: int, start_symbol: int, k: int, temp: float = 1.0) -> Tensor:\n    \"\"\"\n    Function to generate output sequence using top K sampling algorithm\n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        model to decode\n    src: Tensor\n        source tokens\n    src_mask: Tensor\n        mask source tokens\n    max_len: int\n        number of tokens\n    start_symbol: int\n        first symbol of output sequence\n    k: int\n        top number of samples\n    temp: float\n        temperature\n    \n    Return\n    ------\n    Tensor\n    Output sequence\n    \"\"\"\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        prob = prob.div(temp)\n        prob = torch.nn.functional.softmax(prob, dim=1)\n        \n        top_k_prob, top_k_idx = torch.topk(prob, k)\n        top_k_prob = top_k_prob.squeeze(0).cpu()\n        top_k_idx = top_k_idx.squeeze(0).cpu()\n        next_word = torch.multinomial(top_k_prob, 1)[0]\n        next_word = top_k_idx[next_word].item()\n\n        ys = torch.cat([ys,\n                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys","metadata":{"id":"zdI7sA02Nubg","execution":{"iopub.status.busy":"2022-11-17T23:21:48.552501Z","iopub.execute_input":"2022-11-17T23:21:48.552884Z","iopub.status.idle":"2022-11-17T23:21:48.564112Z","shell.execute_reply.started":"2022-11-17T23:21:48.552842Z","shell.execute_reply":"2022-11-17T23:21:48.563057Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"def translate_top_k(model: torch.nn.Module, src_sentence: str, k: int, temp : float = 1.0) -> str:\n    \"\"\"\n    Function to translate input sentence into target language with top k sampling algorithm\n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        pre-trained model used for translate\n    src_sentence: str\n        sentence to translate\n    k: int\n        top number of samples\n    temp: float\n        temperature\n    Return\n    ------\n    str\n    Translated sentence in the target language of the transformer (model)\n    \"\"\"\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = top_k_sampling(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temp=temp).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")","metadata":{"id":"eaXupDoJPYdE","execution":{"iopub.status.busy":"2022-11-17T23:21:48.717771Z","iopub.execute_input":"2022-11-17T23:21:48.718079Z","iopub.status.idle":"2022-11-17T23:21:48.725195Z","shell.execute_reply.started":"2022-11-17T23:21:48.718053Z","shell.execute_reply":"2022-11-17T23:21:48.724045Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"print(\"Target sentence: A group of people stand in front of an igloo.\")\nprint(\"--------------\")\nprint(\"Without temperature variation:\")\nfor k in range(1, 10):\n    print(f\"k = {k} | temp = 1.0:\", translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k))\nprint(\"--------------\")\nprint(\"With temperature variation:\")\nfor k in range(1, 10):\n    for temp in [0.1, 0.3, 0.5, 0.7, 0.9]:\n        print(f\"k = {k} | temp = {temp}:\", translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k, temp))","metadata":{"id":"rP4SmpSgPfrk","execution":{"iopub.status.busy":"2022-11-17T23:21:48.880943Z","iopub.execute_input":"2022-11-17T23:21:48.881203Z","iopub.status.idle":"2022-11-17T23:21:51.244864Z","shell.execute_reply.started":"2022-11-17T23:21:48.881179Z","shell.execute_reply":"2022-11-17T23:21:51.243754Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"Target sentence: A group of people stand in front of an igloo.\n--------------\nWithout temperature variation:\nk = 1 | temp = 1.0:  A group of people standing in front of an igloo \nk = 2 | temp = 1.0:  A group of people stand in front of an igloo \nk = 3 | temp = 1.0:  A group of people in an olive suit . \nk = 4 | temp = 1.0:  A group of people are standing in front of an igloo . \nk = 5 | temp = 1.0:  A group of people stand in front of an igloo \nk = 6 | temp = 1.0:  A group of people standing in front an auditorium . \nk = 7 | temp = 1.0:  A group of people standing in front of an office setting . \nk = 8 | temp = 1.0:  A group of people in front of an abandoned office . \nk = 9 | temp = 1.0:  A group of people stand in front of an igloo . \n--------------\nWith temperature variation:\nk = 1 | temp = 0.1:  A group of people standing in front of an igloo \nk = 1 | temp = 0.3:  A group of people standing in front of an igloo \nk = 1 | temp = 0.5:  A group of people standing in front of an igloo \nk = 1 | temp = 0.7:  A group of people standing in front of an igloo \nk = 1 | temp = 0.9:  A group of people standing in front of an igloo \nk = 2 | temp = 0.1:  A group of people standing in front of an igloo \nk = 2 | temp = 0.3:  A group of people standing in front of an igloo \nk = 2 | temp = 0.5:  A group of people stand in front of an igloo \nk = 2 | temp = 0.7:  A group of people stand in front of an igloo \nk = 2 | temp = 0.9:  A group of people standing in front of an olive office . \nk = 3 | temp = 0.1:  A group of people standing in front of an igloo \nk = 3 | temp = 0.3:  A group of people standing in front of an igloo \nk = 3 | temp = 0.5:  A group of people standing in front of an igloo \nk = 3 | temp = 0.7:  A group of people standing in front of an olive office . \nk = 3 | temp = 0.9:  A group of people stand in front of an auditorium . \nk = 4 | temp = 0.1:  A group of people standing in front of an igloo \nk = 4 | temp = 0.3:  A group of people standing in front of an igloo \nk = 4 | temp = 0.5:  A group of people standing in front of an igloo . \nk = 4 | temp = 0.7:  A group of people standing in front of an igloo \nk = 4 | temp = 0.9:  A group of people stand in front of an igloo \nk = 5 | temp = 0.1:  A group of people standing in front of an igloo \nk = 5 | temp = 0.3:  A group of people standing in front of an igloo \nk = 5 | temp = 0.5:  A group of people standing in front of an igloo \nk = 5 | temp = 0.7:  A group of people stand in front of an igloo \nk = 5 | temp = 0.9:  The group of people are standing in front of an igloo \nk = 6 | temp = 0.1:  A group of people standing in front of an igloo \nk = 6 | temp = 0.3:  A group of people standing in front of an igloo \nk = 6 | temp = 0.5:  A group of people stand in front of an igloo \nk = 6 | temp = 0.7:  A group of people standing in front of an igloo \nk = 6 | temp = 0.9:  A group of people stand in front of an auditorium . \nk = 7 | temp = 0.1:  A group of people standing in front of an igloo \nk = 7 | temp = 0.3:  A group of people stand in front of an igloo . \nk = 7 | temp = 0.5:  A group of people standing in front of an igloo . \nk = 7 | temp = 0.7:  A group of people standing in front of an igloo . \nk = 7 | temp = 0.9:  A group of people stand in front of an igloo \nk = 8 | temp = 0.1:  A group of people standing in front of an igloo \nk = 8 | temp = 0.3:  A group of people standing in front of an igloo \nk = 8 | temp = 0.5:  A group of people standing in front of an igloo \nk = 8 | temp = 0.7:  A group of people stand in front of an auditorium . \nk = 8 | temp = 0.9:  A group of people standing in front of an igloo \nk = 9 | temp = 0.1:  A group of people standing in front of an igloo \nk = 9 | temp = 0.3:  A group of people standing in front of an igloo \nk = 9 | temp = 0.5:  A group of people standing in front of an igloo \nk = 9 | temp = 0.7:  A group of people standing in front of an auditorium . \nk = 9 | temp = 0.9:  A group of people standing in front of an igloo . \n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can observe that more that if `k` is too big, the results are bad, and the `temp` (temperature) seems to give better results when it is low.","metadata":{}},{"cell_type":"markdown","source":"### Beam search algorithm\n\nLet's code now a beam search (from scratch) for the decoding function.","metadata":{"id":"1p2iqh40Q4jJ"}},{"cell_type":"code","source":"def beam_search(model: torch.nn.Module, src: Tensor, src_mask: Tensor, max_len: int, start_symbol: int, beam_size: int) -> Tensor:\n    \"\"\"\n    Function to generate output sequence using beam search algorithm\n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        model to decode\n    src: Tensor\n        source tokens\n    src_mask: Tensor\n        mask source tokens\n    max_len: int\n        number of tokens\n    start_symbol: int\n        first symbol of output sequence\n    beam_size: int\n        number of sample to keep at each step   \n    Return\n    ------\n    Tensor\n    Output sequence\n    \"\"\"\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    memory = memory.to(DEVICE)\n    tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                .type(torch.bool)).to(DEVICE)\n    out = model.decode(ys, memory, tgt_mask)\n    out = out.transpose(0, 1)\n    prob = model.generator(out[:, -1])\n\n    prob = torch.nn.functional.softmax(prob, dim=1)\n    top_k_prob, top_k_idx = torch.topk(prob, beam_size)\n\n    top_k_prob = top_k_prob.squeeze(0).cpu()\n    top_k_idx = top_k_idx.squeeze(0).cpu()\n\n    top_k_prob = top_k_prob.tolist()\n    top_k_idx = top_k_idx.tolist()\n\n    top_k_prob = [[prob, [idx]] for prob, idx in zip(top_k_prob, top_k_idx)]\n\n    next_word = top_k_prob[0][1][0]\n    ys = torch.cat([ys,\n                    torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n\n    for i in range(max_len-1):\n        temp = []\n        for top_prob, top_idx in top_k_prob:\n            ys = torch.tensor(top_idx).view(-1, 1).to(DEVICE)\n            memory = memory.to(DEVICE)\n            tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                        .type(torch.bool)).to(DEVICE)\n            out = model.decode(ys, memory, tgt_mask)\n            out = out.transpose(0, 1)\n            prob = model.generator(out[:, -1])\n            prob = torch.nn.functional.softmax(prob, dim=1)\n            \n            top_k_prob, top_k_idx = torch.topk(prob, beam_size)\n\n            top_k_prob = top_k_prob.squeeze(0).cpu()\n            top_k_idx = top_k_idx.squeeze(0).cpu()\n\n            top_k_prob = top_k_prob.tolist()\n            top_k_idx = top_k_idx.tolist()\n\n            for t_prob, t_idx in zip(top_k_prob, top_k_idx):\n                temp.append([top_prob * t_prob, top_idx + [t_idx]])\n\n        top_k_prob = sorted(temp, key=lambda x: x[0], reverse=True)[:beam_size]\n        for prob, idx in top_k_prob:\n            if idx[:-1] == EOS_IDX:\n                break\n    return torch.tensor(top_k_prob[0][1])","metadata":{"id":"K_B2DFPnv9-k","execution":{"iopub.status.busy":"2022-11-17T23:25:22.505211Z","iopub.execute_input":"2022-11-17T23:25:22.505546Z","iopub.status.idle":"2022-11-17T23:25:22.519863Z","shell.execute_reply.started":"2022-11-17T23:25:22.505517Z","shell.execute_reply":"2022-11-17T23:25:22.518891Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"def translate_beam_search(model: torch.nn.Module, src_sentence: str, beam_size: int) -> str:\n    \"\"\"\n    Function to translate input sentence into target language with beam search algorithm \n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        pre-trained model used for translate\n    src_sentence: str\n        sentence to translate\n    beam_size: int\n        beam size use for decoding function\n    Return\n    ------\n    str\n    Translated sentence in the target language of the transformer (model)\n    \"\"\"\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = beam_search(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, beam_size=beam_size)\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")","metadata":{"id":"mTikaiqPwWES","execution":{"iopub.status.busy":"2022-11-17T23:25:22.962185Z","iopub.execute_input":"2022-11-17T23:25:22.962879Z","iopub.status.idle":"2022-11-17T23:25:22.972042Z","shell.execute_reply.started":"2022-11-17T23:25:22.962807Z","shell.execute_reply":"2022-11-17T23:25:22.971078Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"print(\"Target sentence: A group of people stand in front of an igloo.\")\nprint(\"--------------\")\nprint(\"Beam size variation:\")\nfor size in range(1, 10):\n    print(f\"size = {size}:\", translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", size))","metadata":{"id":"rKGo7nARwaOF","execution":{"iopub.status.busy":"2022-11-17T23:25:23.450135Z","iopub.execute_input":"2022-11-17T23:25:23.451051Z","iopub.status.idle":"2022-11-17T23:25:23.884561Z","shell.execute_reply.started":"2022-11-17T23:25:23.451014Z","shell.execute_reply":"2022-11-17T23:25:23.883542Z"},"trusted":true},"execution_count":135,"outputs":[{"name":"stdout","text":"Target sentence: A group of people stand in front of an igloo.\n--------------\nBeam size variation:\nsize = 1:  A group of people standing in front of an igloo \nsize = 2:  A group of people stand in front of an igloo \nsize = 3:  A group of people in an office setting . \nsize = 4:  A group of people standing in an office setting . \nsize = 5:  A group of people in front of an igloo . \nsize = 6:  A group of people stand in front of a igloo . \nsize = 7:  There is a group of people in an igloo \nsize = 8:  A group of people stand in front an olive mostly suit . \nsize = 9:  A group of people stands in front of an auditorium . \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Compare decoding functions\n\nLet's now qualitatively compare a few (at least 3) translation samples for each approach (even the greedy one).","metadata":{"id":"3liHAQLPSGvO"}},{"cell_type":"code","source":"k = 3\ntemp_top_k = 0.7\nbeam_size = 5\n\ndef compare(model: torch.nn.Module, src_sentence: str, tgt_sentence: str) -> None:\n    \"\"\"\n    Function to compare the performance of greedy, top-k sampling and beam search algorithms\n\n    Parameters\n    ----------\n    model: torch.nn.Module\n        pre-trained transformer model\n    src_sentence: str\n        source sentence to translate\n    tgt_sentence: str\n        expected sentence after translate\n    \"\"\"\n    print(f\"Source sentence: {src_sentence}\")\n    print(f\"Target sentence: {tgt_sentence}\")\n    print(f\"Greedy search: {translate_greedy(model, src_sentence)}\")\n    print(f\"Top-k (k = {k}) sampling without temperature: {translate_top_k(model, src_sentence, k)}\")\n    print(f\"Top-k (k = {k}) sampling with temperature = {temp_top_k}: {translate_top_k(model, src_sentence, k, temp_top_k)}\")\n    print(f\"Beam search (size = {beam_size}): {translate_beam_search(model, src_sentence, beam_size)}\")\n    print(\"------------------\")\n\ncompare(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", \"A group of people stand in front of an igloo.\")\ncompare(transformer, \"Ein Mann in einem gelben Hut, der etwas anstarrt .\", \"A man in a yellow hat staring at something.\")\ncompare(transformer, \"Ich möchte ein Bier.\", \"I want a beer.\")","metadata":{"id":"lDx8YvJSRK1w","execution":{"iopub.status.busy":"2022-11-17T23:25:25.872667Z","iopub.execute_input":"2022-11-17T23:25:25.873035Z","iopub.status.idle":"2022-11-17T23:25:27.068466Z","shell.execute_reply.started":"2022-11-17T23:25:25.873004Z","shell.execute_reply":"2022-11-17T23:25:27.067501Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"Source sentence: Eine Gruppe von Menschen steht vor einem Iglu .\nTarget sentence: A group of people stand in front of an igloo.\nGreedy search:  A group of people standing in front of an igloo \nTop-k (k = 3) sampling without temperature:  Group of people standing in front of an olive office . \nTop-k (k = 3) sampling with temperature = 0.7:  A group of people standing in front of an auditorium . \nBeam search (size = 5): A group of people standing in front of an igloo  .    \n------------------\nSource sentence: Ein Mann in einem gelben Hut, der etwas anstarrt .\nTarget sentence: A man in a yellow hat staring at something.\nGreedy search:  A man in a yellow hat is using something bubble routine . \nTop-k (k = 3) sampling without temperature:  A man in a yellow hat with something tired hair . \nTop-k (k = 3) sampling with temperature = 0.7:  A man in a yellow hat is using something control of them . \nBeam search (size = 5): A man in a yellow hat is making something tea .       \n------------------\nSource sentence: Ich möchte ein Bier.\nTarget sentence: I want a beer.\nGreedy search:  I I see a beer . \nTop-k (k = 3) sampling without temperature:  I see a beer . \nTop-k (k = 3) sampling with temperature = 0.7:  I I I see a beer . \nBeam search (size = 5): I I see a beer  .     \n------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Compute the BLEU score of the model\n\nWe are going to use the sacreBLEU implementation to evaluate our model and quantitatively compare the 4 implemented decoding approaches.","metadata":{"id":"0cmXHDS8U2PZ"}},{"cell_type":"code","source":"from sacrebleu.metrics import BLEU\nimport sacrebleu\n\ndef BLEU_compare(model: torch.nn.Module, sentences: List[str], refs: List[List[str]]) -> None:\n    \"\"\"\n    Apply decoding algorithms on sentences and evaluate them with BLUE score metrics with references sentences\n    \n    Parameters\n    ----------\n    model: torch.nn.Module\n        pre-trained transformer model\n    sentences: List[str]\n        sentences to translate\n    refs: List[List[str]]\n        List of list of references for the expected translations\n    \"\"\"\n    \n    hyps_greedy = (\"Greedy\", [translate_greedy(model, sentence) for sentence in sentences ])\n    hyps_topk_no_temp = (\"Top k sampling without temperature\", [ translate_top_k(model, sentence, k) for sentence in sentences ])\n    hyps_topk_with_temp = (\"Top k sampling with temperature = \"+ str(temp_top_k), [ translate_top_k(model, sentence, k, temp_top_k) for sentence in sentences ])\n    hyps_beam_search = (\"Beam search with size = \" + str(beam_size), [translate_beam_search(model, sentence, beam_size) for sentence in sentences ])\n\n    hyps = [ hyps_greedy , hyps_topk_no_temp, hyps_topk_with_temp, hyps_beam_search]\n\n    # Compute the corpus score\n    bleu = BLEU()\n    for name, h in hyps:\n        res = bleu.corpus_score(h, refs)\n        print(f\"{name:<38}: {str(res)}\")\n\nrefs = [[\n         \"A group of people stand in front of an igloo.\",\n         \"A man in a yellow hat staring at something.\",\n         \"I want a beer.\"\n         ]]\n\nsentences = [\"Eine Gruppe von Menschen steht vor einem Iglu .\", \"Ein Mann in einem gelben Hut, der etwas anstarrt .\", \"Ich möchte ein Bier.\"]\nBLEU_compare(transformer, sentences, refs)","metadata":{"id":"MfhreiXLSLHo","execution":{"iopub.status.busy":"2022-11-17T23:25:28.406068Z","iopub.execute_input":"2022-11-17T23:25:28.408140Z","iopub.status.idle":"2022-11-17T23:25:29.598344Z","shell.execute_reply.started":"2022-11-17T23:25:28.408093Z","shell.execute_reply":"2022-11-17T23:25:29.597224Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"Greedy                                : BLEU = 49.55 75.0/56.0/45.5/31.6 (BP = 1.000 ratio = 1.077 hyp_len = 28 ref_len = 26)\nTop k sampling without temperature    : BLEU = 55.49 71.9/55.2/50.0/47.8 (BP = 1.000 ratio = 1.231 hyp_len = 32 ref_len = 26)\nTop k sampling with temperature = 0.7 : BLEU = 64.55 79.3/65.4/60.9/55.0 (BP = 1.000 ratio = 1.115 hyp_len = 29 ref_len = 26)\nBeam search with size = 5             : BLEU = 54.29 78.6/60.0/50.0/36.8 (BP = 1.000 ratio = 1.077 hyp_len = 28 ref_len = 26)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The output values mean of the corpus score:\n- FIRST NUMBER: final BLEU score\n- NUM/NUM/NUM/NUM: precision value for 1–4 ngram order\n- BP: Brevity Penalty\n- ratio: ratio between the length of hypothesis and reference sentences\n- hyp_len: total number of characters for hypothesis sentences\n- ref_len: total number of characters for reference sentences\n\nWe can observe that with this parameters for top k sampling and beam search, we get better result than the greedy algoritm, as expected.\nWe should benchmark the BLUE score with different parameters for the top k sampling and beam search algorithms, with different k sampling, temperature and beam size.\n\nWe could observe that temperature for top k sampling is usefull to get better results.","metadata":{"id":"Me4WBAglSNKp"}},{"cell_type":"markdown","source":"## References\n\n1. Attention is all you need paper.\n   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n","metadata":{"id":"Ixt5XV6RmHK4"}}]}